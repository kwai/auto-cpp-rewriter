# 背景

以下简单介绍一下背景。

模型数据流的流程如下，广告产生曝光给用户后，用户会发生点击、激活、付费、次留等信号会随客户回传等方式进入广告系统，
通过 user 、item 的构建流程，变成模型能够使用的 user info、item info。线上模型预估服务基于这些信息组装成 `AdLog` 
进行打分，log 再通过 kafka/hive 进入模型训练流程。算法同学日常特征迭代，就是通过对原始数据的加工和变换，将原始的行为
转换为特征能够识别的 sparse 和 dense 特征，这个过程就是我们一般说的特征抽取过程，也叫 feature generation (简称 FG)。

经过多年的发展，商业化模型链路虽然新的特征也使用了 CommonInfoAttr 的 类似 kv 化的方式进行迭代，但是一直使用的是基于
c++ extractor 的模式，这种方式的弊端很明显:
1. `protobuf` 序列化和反序列化开销较大。
2. `protobuf` 的存储 + hard code 的 extracor，导致数据的产出、使用、覆盖率、停更等都难以跟踪和管理，导致模型链路治理非常困难。
3. 数据逻辑与计算逻辑耦合严重，逻辑无法复用。
4. 大量的手写特征，难以标准化，很多有问题的特征，不断被拷贝扩散到很多模型中，历史包袱很重。工程团队的性能优化也难以开展。

为了解决以上这些问题，我们需要对整个特征链路进行彻底的改造。从业界的经验来看，迁移 `kv` 特征是一个比较好的思路。
因此我们跟模型工程、数据工程团队一起对整体链路发起了特征 kv 化改造项目。

项目的目标是通过user info/item info特征数据的kv化改造，实现对特征存储和特征加工逻辑的解耦，建设通用化的特征处理逻辑并全量迁移，
摒弃掉 extractor hard code 的逻辑，实现代码和数据的复用。

主要存在的挑战如下:

1. 模型链路长，特征的构建、模型的 label match 数据流在不同的场景在着多套，如果我们从头到尾建设一条完整的新数据流，
   需要消耗大量的资源，并且对算法团队的迭代形成很大的干扰。
2. 历史包袱非常大。商业化场景多，模型多，存量有 3000+ extractor 实现全量迁移，工作量大，涉及到的团队、人力都非常多。

为了降低迁移成本，我跟工程团队一起制定了如下的整体迁移计划。
1. 第一阶段，user info和item info的 kv 化改造, 将数据格式迁移为 `flatbuffers` 格式。
2. 第二阶段，进行推理侧的 kv 化改造，实现存量 extractor 逻辑的平迁。
3. 第三阶段，训练侧支持 kv 特征，实现最终的闭环。

以上是大的背景，本项目则是在解决第二阶段的存量 extractor 逻辑的平迁问题。

为了保证线上效果，我们需要保证两个版本的数据与处理逻辑完全一致，这样才能保证提取的特征一致。而需要改写
的特征抽取类非常多，数据达到 3000+ 个，且抽取类逻辑并不都是简单逻辑，有些还比较复杂。除此之外，训练侧还用到过滤样本的
`item_filter` 和提取 `label` 的 `label_extracctor`，在第三阶段也需要进行改写，数量也有 `1500+`。如果全部手动改写，
工作量非常巨大，而且排查逻辑一致需要花费大量精力。

因此，我基于 `llvm` 实现了自动改写 `c++` 特征抽取类的工具，自动将抽取类转换为 `kv` 特征抽取类，且保证
逻辑完全一致。
